{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "tf = tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\nBook I\\r\\n\\r\\n \\r\\nChapter 1\\r\\nA Long-expected Party\\r\\n\\r\\nWhen Mr. Bilbo Baggins of Bag End announced that he would shortly be celebrating his eleventy-first birthday with a party of special magnificence, there was much talk and excitement in Hobbiton.\\r\\n\\r\\nBilbo was very rich and very peculiar, and had been the wonder of the Shire for sixty years, ever since his remarkable disappearance and unexpected return. The riches he had brought back from his travels had now become a local legend, and it was popularly believed, whatever the old folk might say, that the Hill at Bag End was full of tunnels stuffed with treasure. And if that was not enough for fame, there was also his prolonged vigour to marvel at. Time wore on, but it seemed to have little effect on Mr. Baggins. At ninety he was much the same as at fifty. At ninety-nine they began to call him well-preserved, but unchanged would have been nearer the mark. There were some that shook their heads and thought this was too much of a good thing; '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('LOTR_Fellowship_Text.txt', 'rb') as fin:\n",
    "    lotr_1_rtext_b = fin.read()\n",
    "lotr_1_rtext = lotr_1_rtext_b.decode()\n",
    "lotr_1_rtext[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_1_rtext.find('\\xa0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-10cb6ebe92ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlotr_1_books\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r\\nBook [IVX]+\\r\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlotr_1_rtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlotr_1_books\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlotr_1_books\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "lotr_1_books = re.split('\\r\\nBook [IVX]+\\r\\n',lotr_1_rtext)[1:]\n",
    "len(lotr_1_books)\n",
    "[x[:100] for x in lotr_1_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_1_book_chapters = [re.split('Chapter \\d+\\r\\n.*\\r\\n', x.strip())[1:] for x in lotr_1_books]\n",
    "[[ch[:100]+'...'+ch[-100:] for ch in bk[:2]+bk[-2:]] for bk in lotr_1_book_chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_1_book_ch_paras = [[re.split('\\r\\n\\r\\n\\s*', ch.strip()) for ch in bk] for bk in lotr_1_book_chapters]\n",
    "lotr_1_book_ch_paras[0][0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilcrow = '¶'\n",
    "section = '§'\n",
    "hyphen = '-' \n",
    "ndash = '\\u2013' #'–'\n",
    "mdash = '—'\n",
    "elipses = '…'\n",
    "print(hyphen, ndash, mdash)\n",
    "hyphen == ndash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be good to concatenate the newlines,\n",
    "# which seem to mess up the later tokenisation, as well as a few other\n",
    "# things: \". . .\" as elipses (utf8 …), and \"--\" as mdash (—)\n",
    "import re\n",
    "def sentence_prep(sentence, debug=False):\n",
    "    sentence = ' '.join(re.split(\"\\s+\", sentence))\n",
    "    sentence = re.sub('\\s*\\. \\. \\.', ' … ',sentence)\n",
    "    sentence = re.sub('----', ' — ',sentence)\n",
    "    sentence = re.sub('---', ' — ',sentence)\n",
    "    sentence = re.sub('--', ' — ',sentence)\n",
    "    sentence = re.sub('“', ' “ ',sentence)\n",
    "    sentence = re.sub('”', ' ” ',sentence)\n",
    "    sentence = re.sub('‘', ' ‘ ',sentence)\n",
    "    sentence = re.sub('’([a-z])', '\\'\\\\1', sentence) #eg \"it isn’t natural\"/\"you’ve\" -> \"isn't\"/\"you've\"\n",
    "    sentence = re.sub('’', ' ’ ',sentence)\n",
    "    return sentence\n",
    "\n",
    "print(sentence_prep(\n",
    "    'This “should” get replaced . . . and even this--one',\n",
    "    debug=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into sentences now\n",
    "# Note full stop chars may not give the full sentences, eg\n",
    "# \"Mr. Bilbo Baggins\"\n",
    "\n",
    "#Import a premade sentence tokeniser\n",
    "import nltk.data\n",
    "\n",
    "#May need to download the data\n",
    "nltk.download('punkt')\n",
    "english_sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x1122ff6e9c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "punkt_tokenizer = PunktSentenceTokenizer(lotr_1_rtext, verbose=1)\n",
    "punkt_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r\\nBook I\\r\\n\\r\\n \\r\\nChapter 1\\r\\nA Long-expected Party\\r\\n\\r\\nWhen Mr. Bilbo Baggins of Bag End announced that he would shortly be celebrating his eleventy-first birthday with a party of special magnificence, there was much talk and excitement in Hobbiton.',\n",
       " 'Bilbo was very rich and very peculiar, and had been the wonder of the Shire for sixty years, ever since his remarkable disappearance and unexpected return.',\n",
       " 'The riches he had brought back from his travels had now become a local legend, and it was popularly believed, whatever the old folk might say, that the Hill at Bag End was full of tunnels stuffed with treasure.',\n",
       " 'And if that was not enough for fame, there was also his prolonged vigour to marvel at.',\n",
       " 'Time wore on, but it seemed to have little effect on Mr. Baggins.',\n",
       " 'At ninety he was much the same as at fifty.',\n",
       " 'At ninety-nine they began to call him well-preserved, but unchanged would have been nearer the mark.',\n",
       " 'There were some that shook their heads and thought this was too much of a good thing; it seemed unfair that anyone should possess (apparently) perpetual youth as well as (reputedly) inexhaustible wealth.',\n",
       " '‘It will have to be paid for,’ they said.',\n",
       " '‘It isn’t natural, and trouble will come of it!’\\r\\n\\r\\nBut so far trouble had not come; and as Mr. Baggins was generous with his money, most people were willing to forgive him his oddities and his good fortune.',\n",
       " 'He remained on visiting terms with his relatives (except, of course, the Sackville-Bagginses), and he had many devoted admirers among the hobbits of poor and unimportant families.',\n",
       " 'But he had no close friends, until some of his younger cousins began to grow up.',\n",
       " 'The eldest of these, and Bilbo’s favourite, was young Frodo Baggins.',\n",
       " 'When Bilbo was ninety-nine, he adopted Frodo as his heir, and brought him to live at Bag End; and the hopes of the Sackville-Bagginses were finally dashed.',\n",
       " 'Bilbo and Frodo happened to have the same birthday, September 22nd.',\n",
       " '‘You had better come and live here, Frodo my lad,’ said Bilbo one day; ‘and then we can celebrate our birthday-parties comfortably together.’ At that time Frodo was still in his tweens, as the hobbits called the irresponsible twenties between childhood and coming of age at thirty-three.',\n",
       " 'Twelve more years passed.',\n",
       " 'Each year the Bagginses had given very lively combined birthday-parties at Bag End; but now it was understood that something quite exceptional was being planned for that autumn.',\n",
       " 'Bilbo was going to be eleventy-one, 111, a rather curious number and a very respectable age for a hobbit (the Old Took himself had only reached 130); and Frodo was going to be thirty-three, 33) an important number: the date of his ‘coming of age’.',\n",
       " 'Tongues began to wag in Hobbiton and Bywater; and rumour of the coming event travelled all over the Shire.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_prep_bk_ch_para_sens = [[\n",
    "    [\n",
    "        [sentence_prep(sen, debug=False) for sen in english_sentence_tokenizer.tokenize(para)]\n",
    "        for para in ch\n",
    "    ] for ch in lotr_book\n",
    "] for lotr_book in lotr_1_book_ch_paras]\n",
    "#aoav_prep_paras = [\n",
    "#    sentence_prep(para, debug=False)\n",
    "#    for ch in aoav_ch_paras\n",
    "#    for para in ch\n",
    "#]\n",
    "lotr_prep_bk_ch_para_sens[0][0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( '\\n-----\\n'.join(lotr_prep_bk_ch_para_sens[0][0][1][:]) )\n",
    "print('===')\n",
    "print( '\\n-----\\n'.join(lotr_prep_bk_ch_para_sens[1][6][0][:]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_all_stringable_sentences = []\n",
    "for book in lotr_prep_bk_ch_para_sens:\n",
    "    for ch in book:\n",
    "        sen_list = []\n",
    "        for para in ch:\n",
    "            for sen in para:\n",
    "                sen_list.append(sen)\n",
    "        lotr_all_stringable_sentences.append(sen_list)\n",
    "lotr_all_stringable_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start using tf for tokenization etc as well.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<UNKNOWN>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_all_sentences_strung = []\n",
    "for sentence_list in lotr_all_stringable_sentences:\n",
    "    lotr_all_sentences_strung.extend(sentence_list)\n",
    "\n",
    "lotr_all_sentences_strung[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d total sentences\" % len(lotr_all_sentences_strung))\n",
    "tokenizer.fit_on_texts(lotr_all_sentences_strung)\n",
    "total_words = len(tokenizer.word_index) + 1 #word_index is a dict\n",
    "print(\"%d total words\" % total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect how this processes:\n",
    "test_sentence = lotr_all_sentences_strung[2] + ' - then an OOV word and mdash: ' + mdash\n",
    "print(test_sentence)\n",
    "tseq = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "print(tseq)\n",
    "print(tokenizer.sequences_to_texts([tseq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 - try to make sentences by adding the full stop punctuation\n",
    "DOT_STR = \"·\" #One that wont get filtered by the default tokenisation\n",
    "tokenizer.fit_on_texts([DOT_STR])\n",
    "\n",
    "DOT_TOKEN = tokenizer.texts_to_sequences([DOT_STR])[0][0]\n",
    "print(DOT_STR, \"->\", DOT_TOKEN)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1 #word_index is a dict\n",
    "print(\"%d total words\" % total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram - note that paragraphs are stringable, but probably best to exclude chapters\n",
    "## modified From the colab:\n",
    "# create input sequences using list of tokens\n",
    "\n",
    "DOT_SENTENCES = True #Whether to include a dot token between sentences\n",
    "\n",
    "token_strings_list = []\n",
    "for stringable_sentences in lotr_all_stringable_sentences:\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(stringable_sentences)\n",
    "    #print(stringable_sentences, tokenized_sentences)\n",
    "    #break\n",
    "    tseq = []\n",
    "    for tlist in tokenized_sentences:\n",
    "        tseq.extend(tlist)\n",
    "        if DOT_SENTENCES:\n",
    "            tseq.append(DOT_TOKEN)\n",
    "    token_strings_list.append(tseq)\n",
    "\n",
    "include_sentences_alone = True\n",
    "if include_sentences_alone:\n",
    "    for sentence_list in lotr_all_stringable_sentences:\n",
    "        tokenized_sentences = tokenizer.texts_to_sequences(sentence_list)\n",
    "        token_strings_list.extend(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence_len = max([(len(x),x) for x in token_strings_list])\n",
    "print(\n",
    "    len(token_strings_list), \"sentence lists\", \",\",\n",
    "    longest_sentence_len[0], \"is longest len\", \" --> \",\n",
    "    longest_sentence_len[1][:30],\n",
    "    tokenizer.sequences_to_texts([longest_sentence_len[1][:30]])[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ought to limit the back check, maybe to 30 words\n",
    "MAX_ALLOWED_SEQ_LENGTH = 30 #then there is 1 for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences=[]\n",
    "for tseq in token_strings_list:\n",
    "    for i in range(len(tseq)):\n",
    "        #For each, collect the ngrams\n",
    "        #And collect the [0:n] token 'n-grams'\n",
    "        #  (feed in partial sentences as well)\n",
    "        n_gram_sequence = tseq[max(0,i-MAX_ALLOWED_SEQ_LENGTH):i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "    #print(tseq, n_gram_sequence, len(n_gram_sequence))\n",
    "    #break\n",
    "\n",
    "input_sequences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_sentences[:2])\n",
    "print(input_sequences[:20])\n",
    "print(max(len(x) for x in input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the sequences by padding them (left side zeros / \"pre-padding\")\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "input_sequences_padded = np.array(\n",
    "    pad_sequences(\n",
    "        input_sequences,\n",
    "        maxlen=MAX_ALLOWED_SEQ_LENGTH,\n",
    "        padding='pre',\n",
    "    )\n",
    ")\n",
    "input_sequences_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is enough for the prep.\n",
    "# Now lets save, and then the data can be input into the processing script\n",
    "#Need to save the tokeniser as well for encoding:\n",
    "# then can use eg: tokenizer.texts_to_sequences([the_sentence])[0]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('lotr_data_prep.pickle', 'wb') as handle:\n",
    "    pickle.dump({\n",
    "            'tokenizer': tokenizer,\n",
    "            'train_tokens': input_sequences_padded,\n",
    "        },\n",
    "        handle,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
